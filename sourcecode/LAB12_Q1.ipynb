{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAB12_Q1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5gF-E6aP21a"
      },
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "waUqQ6NXS4Mf",
        "outputId": "0ed6471e-8d8f-4878-ce97-20fb70ff483e"
      },
      "source": [
        "# load data and keeping the needed data columns \n",
        "data = pd.read_csv('/content/Sentiment.csv')\n",
        "data = data[['text', 'sentiment']]\n",
        "data.head(5) # looking at the data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text sentiment\n",
              "0  RT @NancyLeeGrahn: How did everyone feel about...   Neutral\n",
              "1  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
              "2  RT @TJMShow: No mention of Tamir Rice and the ...   Neutral\n",
              "3  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
              "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sYrW2XQTQMV"
      },
      "source": [
        "data['text'] = data['text'].apply(lambda x: x.lower()) # lowercase text\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "for idx, row in data.iterrows(): # iterate through the data and replace the rows with rt with ' ' \n",
        "  row[0] = row[0].replace('rt', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4I7y9-DUCot"
      },
      "source": [
        "max_features = 2000 # creating max feauture variable \n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ') #passing in the number of max features and splitting the data\n",
        "\n",
        " # getting the values fitting the tokenizer \n",
        "tokenizer.fit_on_texts(data['text'].values) \n",
        "X = tokenizer.texts_to_sequences(data['text'].values) \n",
        "X = pad_sequences(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD2CC8LtVMy9",
        "outputId": "982a4586-7901-48ec-c853-756c8185d3bd"
      },
      "source": [
        "embed_dim = 128 # embedding dimensions\n",
        "lstm_out = 196 # lst output\n",
        "# model created\n",
        "def createmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "    model.save('model.h5') # saving the model \n",
        "\n",
        "    return model\n",
        "\n",
        "model_sum = createmodel()\n",
        "model_sum.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 28, 128)           256000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 591       \n",
            "=================================================================\n",
            "Total params: 511,391\n",
            "Trainable params: 511,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11xtRg9WWBEz",
        "outputId": "25dce7e4-0dc7-4a66-a4fc-55dff27a9335"
      },
      "source": [
        "# label encoding the senteiment columns \n",
        "labelencoder = LabelEncoder() \n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42) # traning the data with test size of 0.33\n",
        "\n",
        "# compiling the model \n",
        "batch_size = 32\n",
        "model = createmodel()\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)\n",
        "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size)\n",
        "# looking at the score\n",
        "print(score)\n",
        "print(acc)\n",
        "print(model.metrics_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "291/291 - 46s - loss: 0.8268 - accuracy: 0.6451\n",
            "144/144 - 3s - loss: 0.7806 - accuracy: 0.6562\n",
            "0.7806485295295715\n",
            "0.6561817526817322\n",
            "['loss', 'accuracy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVOa0URhZYbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52337032-9cd5-432f-e67e-68b7127d6b7e"
      },
      "source": [
        "#loading model model.h5 and predictng the model using the sentence provided in the ICP\n",
        "from keras.models import load_model\n",
        "loaded_model = load_model('/content/model.h5')\n",
        "my_text = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing.\"\n",
        "\n",
        "new_tokenizer = Tokenizer(num_words=max_features, split=' ') #passing in the number of max features and splitting the data\n",
        "\n",
        " # getting the values fitting the tokenizer \n",
        "new_tokenizer.fit_on_texts(my_text) \n",
        "X = new_tokenizer.texts_to_sequences(my_text) \n",
        "X = pad_sequences(X)\n",
        "loaded_model.predict([X])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 1).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33096087, 0.33656436, 0.3324748 ],\n",
              "       [0.33426136, 0.33163586, 0.33410278],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33426136, 0.33163586, 0.33410278],\n",
              "       [0.33420375, 0.33065507, 0.33514115],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33417854, 0.33335453, 0.3324669 ],\n",
              "       [0.33426136, 0.33163586, 0.33410278],\n",
              "       [0.33426136, 0.33163586, 0.33410278],\n",
              "       [0.3334536 , 0.33411723, 0.33242917],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.33282185, 0.334315  , 0.33286315],\n",
              "       [0.33263457, 0.33159095, 0.33577448],\n",
              "       [0.33373022, 0.3333492 , 0.33292055],\n",
              "       [0.33417854, 0.33335453, 0.3324669 ],\n",
              "       [0.33441266, 0.33116743, 0.3344199 ],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.33400682, 0.33236727, 0.3336259 ],\n",
              "       [0.33140785, 0.33526167, 0.33333048],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33282185, 0.334315  , 0.33286315],\n",
              "       [0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.3330931 , 0.33255216, 0.3343547 ],\n",
              "       [0.3330931 , 0.33255216, 0.3343547 ],\n",
              "       [0.33140785, 0.33526167, 0.33333048],\n",
              "       [0.33373022, 0.3333492 , 0.33292055],\n",
              "       [0.33263457, 0.33159098, 0.33577448],\n",
              "       [0.33373022, 0.3333492 , 0.33292055],\n",
              "       [0.33417854, 0.33335453, 0.3324669 ],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33001873, 0.3341034 , 0.33587793],\n",
              "       [0.33140785, 0.33526167, 0.33333048],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.33400682, 0.33236727, 0.3336259 ],\n",
              "       [0.33140785, 0.33526167, 0.33333048],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33400682, 0.33236727, 0.3336259 ],\n",
              "       [0.33140785, 0.33526167, 0.33333048],\n",
              "       [0.33441266, 0.33116743, 0.3344199 ],\n",
              "       [0.3330931 , 0.33255216, 0.3343547 ],\n",
              "       [0.33140785, 0.33526167, 0.33333048],\n",
              "       [0.33377996, 0.3345772 , 0.33164284],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.33140785, 0.33526167, 0.33333048],\n",
              "       [0.3334536 , 0.33411723, 0.33242917],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.33417854, 0.33335453, 0.3324669 ],\n",
              "       [0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.33263457, 0.33159095, 0.33577448],\n",
              "       [0.33373022, 0.3333492 , 0.33292055],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.33282185, 0.334315  , 0.33286315],\n",
              "       [0.33400682, 0.33236727, 0.3336259 ],\n",
              "       [0.33426136, 0.33163586, 0.33410278],\n",
              "       [0.33338335, 0.33279184, 0.33382484],\n",
              "       [0.33417854, 0.33335453, 0.3324669 ],\n",
              "       [0.33282185, 0.334315  , 0.33286315],\n",
              "       [0.33426136, 0.33163586, 0.33410278],\n",
              "       [0.33338335, 0.33279184, 0.33382484],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.33282185, 0.334315  , 0.33286315],\n",
              "       [0.33140785, 0.33526167, 0.33333048],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33001873, 0.3341034 , 0.33587793],\n",
              "       [0.33426136, 0.33163586, 0.33410278],\n",
              "       [0.33400682, 0.33236727, 0.3336259 ],\n",
              "       [0.33096087, 0.33656436, 0.3324748 ],\n",
              "       [0.3334536 , 0.33411723, 0.33242917],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.33373022, 0.3333492 , 0.33292055],\n",
              "       [0.3334536 , 0.33411723, 0.33242917],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.33282185, 0.334315  , 0.33286315],\n",
              "       [0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.33451232, 0.3309844 , 0.33450332],\n",
              "       [0.33441266, 0.33116743, 0.3344199 ],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.33417854, 0.33335453, 0.3324669 ],\n",
              "       [0.33400682, 0.33236724, 0.3336259 ],\n",
              "       [0.33140785, 0.33526167, 0.33333048],\n",
              "       [0.33283338, 0.3354214 , 0.3317452 ],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.3357007 , 0.33065656, 0.33364278],\n",
              "       [0.3340708 , 0.33195505, 0.3339742 ],\n",
              "       [0.33282185, 0.334315  , 0.33286315],\n",
              "       [0.33263457, 0.33159095, 0.33577448],\n",
              "       [0.33373022, 0.3333492 , 0.33292055],\n",
              "       [0.33417854, 0.33335453, 0.3324669 ],\n",
              "       [0.3357007 , 0.33065656, 0.33364278]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}